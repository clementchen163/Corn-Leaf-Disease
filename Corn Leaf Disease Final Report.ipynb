{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc77b75",
   "metadata": {},
   "source": [
    "![title slide](Images/Title_Slide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0ab4b",
   "metadata": {},
   "source": [
    "## Corn Leaf Disease Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e9615",
   "metadata": {},
   "source": [
    "The United States is the largest producer of corn in the world. As a staple crop, corn plays a\n",
    "large role in the economy as a key ingredient in many manufactured goods including non-food products. Being able to differentiate between healthy corn leaves and different diseases early on can allow for quick and effective treatment, mitigating any potential losses or food security issues. Here we examine images of corn leaves that have been affected by either Blight, Common Rust, Gray Leaf Spot, or are Healthy. The goal of this analysis is to create a Convolutional Neural Network(CNN) that is able to classify different corn diseases based on labeled images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e9ce8",
   "metadata": {},
   "source": [
    "## Who Would Find this Analysis Useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93c8c2",
   "metadata": {},
   "source": [
    "Corn is the largest grain export of the United States with over 18 Billion USD in export value in 2021. As such, a large part of the population live in economies revolving around corn or within adjacent industries. Specifically corn farmers, many of whom reside in the mid-west would find this analysis useful in order to quickly and effectively eliminate diseases that would cripple their crop. Players in corn adjacent industries such as giant food and beverage corporations like Pepsi-Co, Nestle, Coco-Cola, and Kellog's whose products use many corn derivatives would be interested in ensuring their supply remain steady."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a028b0",
   "metadata": {},
   "source": [
    "![map of corn](Images/corn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a4041",
   "metadata": {},
   "source": [
    "## 0. Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71db5b",
   "metadata": {},
   "source": [
    "This dataset has been made from a combination of images from the PlantDoc and PlantVillage datasets.\n",
    "PlantDoc images were webscraped and manually labeled while PlantVillage images are taken mostly within laboratory settings. In total, we have 4188 images from 4 different classes. 1146 images in the Blight class, 1306 images in the Common Rust class, 574 images in the Gray Leaf Spot class, and 1162 images in the Healthy class. The images are all 3 channel RGB jpeg images but have varying dimensions. The images often have varying backgrounds, lightning, show part of a leaf or multiple plants. Some images are partially occluded by human fingers. The dataset and associated papers are linked below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4456e",
   "metadata": {},
   "source": [
    "* [Corn Leaf Dataset](https://www.kaggle.com/datasets/smaranjitghose/corn-or-maize-leaf-disease-dataset)\n",
    "\n",
    "* [PlantDoc Paper](https://arxiv.org/pdf/1911.10317.pdf)\n",
    "\n",
    "* [PlantVillage Dataset](https://www.kaggle.com/datasets/emmarex/plantdisease?datasetId=70909&sortBy=voteCount)\n",
    "\n",
    "* [Project Data](https://github.com/clementchen163/Corn-Leaf-Disease/tree/main/0.%20Project%20Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a5871",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling and Exploratory Data Analysis\n",
    "---\n",
    "[Data Wrangling and Exploratory Data Analysis Report](https://github.com/clementchen163/Corn-Leaf-Disease/blob/main/1.%20Data%20Wrangling%20and%20EDA/Data%20Wrangling%20and%20EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bf6cb",
   "metadata": {},
   "source": [
    "Our data consist of 4188 RGB jpeg images with dimensions ranging from 116-5134 pixels. Because convolutional nueral networks require input images be of the same size in order for the matrix math to work properly, we need to modify our images before training. As we can see below, the raw images are of varying dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed955da2",
   "metadata": {},
   "source": [
    "![Raw Blight Examples](Images/Raw_Blight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1973d82",
   "metadata": {},
   "source": [
    "#### Image Width and Height Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abb9ae",
   "metadata": {},
   "source": [
    "We can see from the histograms below that an overwhelming majority of the images have a height or width of 256 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bad7d3",
   "metadata": {},
   "source": [
    "![Widths Distibution](Images/width_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d7ed7",
   "metadata": {},
   "source": [
    "![Height Distribution](Images/height_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ce221",
   "metadata": {},
   "source": [
    "It turns out that ~ 92.3% of our images have a width of 256 pixels or less\n",
    "and ~ 92.7% of our images have a height of 256 pixels or less\n",
    "with ~ 92.0% of images have a height and width equal to 256 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461c1e5",
   "metadata": {},
   "source": [
    "Since so many of our images have dimensions of 256 x 256, maybe it would make sense to use those dimensions for inputing into our CNN. \n",
    "But what about the dimensions of all the images that aren't 256 x 256? Below we have a scatter plot of image height and width for images without the 256 x 256 dimensions. The red line is the line of equality. We can see that the majority of the non 256 x 256 images are \"near-square\" because of their proximity to the line of equality. Almost all of the images have at least one dimension larger than 256 pixels. Because larger image sizes would require more computational resources to train the model, it would make most sense to downscale these images to be 256 x 256 as this would have the least negative impact because most images are already at this resolution or are near-square."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d66bf0",
   "metadata": {},
   "source": [
    "![non square scatter](Images/nonsquare_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfe7da",
   "metadata": {},
   "source": [
    "#### Image Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbdd75",
   "metadata": {},
   "source": [
    "We can see that input images have varying width and height dimensions. However, Covolutional Neural Networks (CNNs) require that input images all be of the same size in order for the matrix math to work correctly. There are several options to satisfy these requirements although some may have negative effects on training.\n",
    "\n",
    "• **Cropping**\n",
    "\n",
    "Cropping images means selecting a window size and centering it around the most important parts of the image. This introduces some new problems: deciding what part of the image is most important as well as the fact that certain border pixels are going to be removed resulting in potential loss of information. In addition, the wide variety of image dimensions (116 pixels to 5184 pixels) means a cropping widow for one image might not work as well for another image.\n",
    "    \n",
    "• **Stretching**\n",
    "\n",
    "Stretching images means stretching or squashing the image to reach the desired dimensions. This results in less loss of information than cropping but features may be distorted. If the image is squashed, some information is loss due to the reduction in granularity. For both stretching and squashing, the relative spatial orientation of pixels becomes distorted which could interfere with the CNN's ability to discern important features.\n",
    "    \n",
    "• **Zero-padding**\n",
    "\n",
    "Zero-padding is our last solution and keeps original image aspect ratio. In order to convert each image to the same size, images are first upscaled or downscaled until the longest dimension is 256 pixels (or whatever you choose) while maintaining the aspect ratio. Pixels of 0's are then filled around the shorter side's border to make the resulting image square. A 2019 [study](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0263-7) showed that \"zero-padding had no effect on the classification accuracy but considerably reduced the training time. The reason is that neighboring zero input units (pixels) will not activate their corresponding convolutional unit in the next layer. Therefore, the synaptic weights on outgoing links from input units do not need to be updated if they contain a zero value.\"\n",
    "\n",
    "\n",
    "**Zero-padding** seems like the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2548d9",
   "metadata": {},
   "source": [
    "#### Resizing and Zero-Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff25e2",
   "metadata": {},
   "source": [
    "For images not at 256 x 256, we first need to resize them by upscaling or downscaling while maintaining the aspect ratio to prevent stretching or squeezing that could destroy important features. Then to make the image square, we zero-pad the sides with black pixels ((0, 0, 0) in RGB). Below are some example images after resizing and zero-padding. The aspect ratios are maintained so the images are not distorted but fit our requirement of all being of size 256 x 256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7ae21",
   "metadata": {},
   "source": [
    "![zero padded blight images](Images/zero-padded_blight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced586f1",
   "metadata": {},
   "source": [
    "#### Class RGB Image Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e9cfe",
   "metadata": {},
   "source": [
    "After rescaling and zero-padding all of our images, it is now possible to average our images by taking the elementwise mean. By averaging the images within each class, we lose certain information such as locations of specific translationally variant features like spots or miscolored stripes. However, we can see the shading of different colors between the classes. The most notable difference is between the diseased classes of Blight, Common Rust, Gray Leaf Spot which all have a dark brownish green color versus the Healthy class which has a much brighter vibrant green. A simpler binary classification task of labeling diseased versus healthy leaves could yield very good results as these classes have very different easily seperarble color schemes as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15d269",
   "metadata": {},
   "source": [
    "![average blight](Images/average_blight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da6521",
   "metadata": {},
   "source": [
    "![average common rust](Images/average_common_rust.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990078f",
   "metadata": {},
   "source": [
    "![average gray leaf spot](Images/average_gray_leaf_spot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d9b10",
   "metadata": {},
   "source": [
    "![average healthy](Images/average_healthy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8185f6",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Modeling\n",
    "---\n",
    "[Preprocessing and Modeling Report](https://github.com/clementchen163/Corn-Leaf-Disease/tree/main/2.%20Preprocessing%20and%20Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf6c29",
   "metadata": {},
   "source": [
    "#### Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bced4",
   "metadata": {},
   "source": [
    "Data augmention is the process of using existing data, augmenting it slightly to create 'new' instances. This can help increase the number of data examples to train from reducing overfitting. For our CNNs, we place a data augmention layer after the input which randomly applies a horizontal flip, rotates the images up to +/- 10° and zooms in or out by up to +/- 20%. By doing this, one image can be augmented into many other, slightly different images as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee9968",
   "metadata": {},
   "source": [
    "![data augmentation](Images/data_augmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f84667",
   "metadata": {},
   "source": [
    "## Original Data vs Zero-Padded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb2a5c",
   "metadata": {},
   "source": [
    "To demonstrate the effectiveness of the reshaped and zero-padded data, we train 2 basic CNNs, one using the raw original data that has been stretched so all images are the same size, and the second using the reshaped and zero-padded data we engineered. Both CNNs use the same architecture shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b85b3f",
   "metadata": {},
   "source": [
    ">* Input Images\n",
    ">\n",
    ">* Data Augmentation\n",
    ">\n",
    ">* Rescaling Pixel Values from [0-255] to [0-1]\n",
    ">\n",
    ">* Convolutional Layer (filters = 32, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 64, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 128, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 256, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 256, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Flatten\n",
    ">\n",
    ">* Densely Connected Layer (units = 4, activation = 'softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f27a5",
   "metadata": {},
   "source": [
    "#### Original Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e81822",
   "metadata": {},
   "source": [
    "The CNN using the original data reaches a validation accuracy of ~ 90% before starting to overfit around epoch 65."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af041b",
   "metadata": {},
   "source": [
    "![training and validation accuracy original data](Images/original_data_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d038dbe",
   "metadata": {},
   "source": [
    "#### Zero-Padded Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4edf8f",
   "metadata": {},
   "source": [
    "The same CNN but trained on the reshaped and zero-padded data reaches a validation accuracy of ~93% before starting to overfit around epoch 75. What this tells us is that the zero-padded data has more potential for higher accuracy but is slower to learn with. Maybe we can fix this weakness of the zero-padded data by using part of a pretrained CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e1ddb",
   "metadata": {},
   "source": [
    "![training and validation accuracy zero-padded data](Images/padded_data_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a9814",
   "metadata": {},
   "source": [
    "## Transfer Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa0342",
   "metadata": {},
   "source": [
    "According to François Chollet, the creator of the Keras machine learning library:\n",
    ">\"A pretrained model is a model that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, the spatial hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world, and hence, its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task.\" \n",
    ">\n",
    "\n",
    "We see in our previous models that it takes quite a few epochs before the models start overfitting which means our model takes quite some time to learn the \"generic features of the visual world\" before it can then learn how to classify the different corn diseases. Here we use part of a pre-trained CNN model(VGG16) that has been trained on the ImageNet dataset which contains over 14 million images. The plan is to take the convolutional base from the VGG16 model which has learned some small features general to the visual world and discard the densely connected classification layers which have been tuned to classify animals and other objects. In its place we would train our own densely connected layers to tailor its application to our corn leaf disease classification task. So ontop of the convolutional base, we add some densely connected layers along with a dropout layer with a softmax activation function output layer. Before we start any training, we freeze the weights on the convolutional base because if we don't, the randomly initialized weights of the densely connected layers will backpropogate and destroy all the learning from our pretrained convolutional base once we begin training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a848f",
   "metadata": {},
   "source": [
    "#### The Convolutional Base of the VGG16 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27970f3",
   "metadata": {},
   "source": [
    "The convolutional base has repeating blocks in a pyramid-like structure. The repeated blocks have a  general conv-conv-pooling structure which have increasingly many filters. The weights and filters in these layers are frozen so that the backpropagation from the randomly initalized weights of the densely connected layers don't destroy the learning already in the convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e77466",
   "metadata": {},
   "source": [
    ">* Input Images\n",
    ">\n",
    ">* Data Augmentation\n",
    ">\n",
    ">* Rescaling Pixel Values from [0-255] to [0-1]\n",
    ">\n",
    ">* Convolutional Layer (filters = 64, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 64, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 128, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 128, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 256, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 256, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 256, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)\n",
    ">\n",
    ">* Convolutional Layer (filters = 512, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 512, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Convolutional Layer (filters = 512, kernel_size = 3, activation = 'relu')\n",
    ">\n",
    ">* Max Pooling Layer (pool_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a79fd",
   "metadata": {},
   "source": [
    "#### Densely Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fda23",
   "metadata": {},
   "source": [
    "The densely connected layers has inbetween them a dropout of 10% to prevent over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fea6d",
   "metadata": {},
   "source": [
    ">* Flatten\n",
    ">\n",
    ">* Densely Connected Layer (units = 256))\n",
    ">\n",
    ">* Dropout (rate = 0.1)\n",
    ">\n",
    ">* Densely Connected Layer (units = 4, activation = 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ebe268",
   "metadata": {},
   "source": [
    "We can see that with the pretrained model, the accuracy starts very high in the low 90s and begins over fitting almost immediately. What this tells us is that even with randomly initalized weights in the densely connected layers, by the fourth epoch we reach a validation accuracy of ~93%. This means our convolutional base taken from the pretrained model is doing very well. What we need to do now is optimize the hyperparameters of our densely connected layers and try to get some model improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b4e9c",
   "metadata": {},
   "source": [
    "![transfer learning accuracy](Images/transfer_learning_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3540a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning of the Transfer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724205a",
   "metadata": {},
   "source": [
    "The above results of the inital transfer model look promising. The densely connected and dropout layers we added on top of the convolutional base had hyperparameters chosen arbitrarily. We can choose better ones by doing some hyperparameter tuning. \n",
    "\n",
    "Due to limited computational resources, the hyperparameter search space will be restricted to the number of nodes in the first dense layer in the range (128, 512) in steps of 32 as well as dropout percentage in the range (0.1, 0.5) in steps of 0.2 with a total of 39 possible combinations.\n",
    "\n",
    "With a selected pair of search hyperparameters, a model is trained and evaulated using validation data and the next pair of search hyperparameters are chosen using a bayesian optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea7f48",
   "metadata": {},
   "source": [
    "After going through the search space, our winning model had hyperparameters with 128 units in the first densely connected layer and a dropout of 0.5. Both these values are at the limits of their respective search space which leads me to believe we may have reached a local optima and not the global optima due to inital restrictions on our search space. Due to lack of computational resources (it takes a long time to search through hyperparameters) we will continue forward but this could be room for future improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55ae57",
   "metadata": {},
   "source": [
    "#### Top 5 Hyperparameter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f4f3",
   "metadata": {},
   "source": [
    "With the top 5 hyperparameter models we obtained from the search, we want to optimize the number of epochs that these models train for. Training for too many epochs could lead to overfitting on the training data especially with the transfer model we have. Looking back we can see how quickly the transfer learning model starts overfitting. So we take these 5 hyperparameters and retrain them again but this time looking for an optimal number of training epochs. Below we have the training histories of each of the 5 chosen models (1 being the best and 5 worst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cbf22",
   "metadata": {},
   "source": [
    "Model 1 reaches a peak validation accuracy of ~95% before overfitting around epoch 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ef825",
   "metadata": {},
   "source": [
    "![hypermodel 1](Images/hypermodel1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1550a",
   "metadata": {},
   "source": [
    "Model 2 reaches a peak validation accuracy of ~95% before overfitting around epoch 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc1bf8",
   "metadata": {},
   "source": [
    "![hypermodel 2](Images/hypermodel2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec338918",
   "metadata": {},
   "source": [
    "Model 3 reaches a peak validation accuracy of ~93% before overfitting around epoch 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ceb82",
   "metadata": {},
   "source": [
    "![hypermodel 3](Images/hypermodel3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb8c4b",
   "metadata": {},
   "source": [
    "Model 4 reaches a peak validation accuracy of ~94% before overfitting around epoch 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c135d7d",
   "metadata": {},
   "source": [
    "![hypermodel 4](Images/hypermodel4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5c6d6",
   "metadata": {},
   "source": [
    "Model 5 reaches a peak validation accuracy of ~95% before overfitting around epoch 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209fa52",
   "metadata": {},
   "source": [
    "![hypermodel 5](Images/hypermodel5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d1492",
   "metadata": {},
   "source": [
    "Once we get the best number of epochs, we retrain the 5 models one last time with their hyperparameters and best number of epochs. We use the combined training and validation set just so we have the model see more data (we no long have a need for the validation set). Because more data is being used, we allow for the models to train for (1.1 * best#of epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a00ebd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43853d03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23a5ce1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2431dd7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14671ee8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0b2b6c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
